"""
Gestor del modelo Florence‚Äë2 (versi√≥n parcheada para RTX¬†4090 y dtype coherente)
"""

from pathlib import Path
from unittest.mock import patch

import gc
import os
import yaml

import torch
from transformers import AutoModelForCausalLM, AutoProcessor
from transformers.dynamic_module_utils import get_imports

# Activar optimizaciones Tensor¬†Cores (TF32) ‚Äì opcional, no afecta al dtype elegido
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


class Florence2Manager:
    """Carga y gestiona un checkpoint local de Florence‚Äë2."""

    def __init__(self, model_path: str | None = None):
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if model_path is not None:
            self.model_id = str(Path(model_path).resolve())
        else:
            self.model_id = self._leer_model_path()

    def _leer_model_path(self) -> str:
        """Obtiene la ruta del modelo desde el archivo de configuraci√≥n."""
        try:
            with open("config/settings.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            ruta = config.get("modelo", {}).get(
                "ruta_local", "modelos/Florence-2-large.safetensors"
            )
            return str(Path(ruta).resolve())
        except Exception:
            return str(Path("modelos/Florence-2-large.safetensors").resolve())

    # ---------------------------------------------------------------------
    #  Utilidad para evitar que Flash‚ÄëAttn se cargue si no est√° disponible
    # ---------------------------------------------------------------------
    def _fixed_get_imports(self, filename):
        imports = get_imports(filename)
        if str(filename).endswith("modeling_florence2.py") and "flash_attn" in imports:
            imports = [imp for imp in imports if imp != "flash_attn"]
        return imports


    # ---------------------------------------------------------------------
    #  Carga de modelo y processor
    # ---------------------------------------------------------------------
    def _verificar_ruta_modelo(self, callback=None):
        """Valida que ``self.model_id`` apunte a una carpeta de modelo."""
        ruta = Path(self.model_id)
        if ruta.is_file():
            if ruta.suffix == ".safetensors":
                self.model_id = str(ruta.parent)
            else:
                mensaje = f"‚ùå Se esperaba una carpeta, no el archivo '{ruta}'"
                if callback:
                    callback(mensaje)
                raise ValueError(mensaje)
        elif not ruta.is_dir():
            mensaje = f"‚ùå Ruta del modelo no v√°lida: '{ruta}'"
            if callback:
                callback(mensaje)
            raise FileNotFoundError(mensaje)

    def cargar_modelo(self, callback=None):
        """Carga Florence‚Äë2 en **float32** para evitar errores de tipo mezclado."""
        try:
            self._verificar_ruta_modelo(callback)
            if callback:
                callback("üß† Cargando modelo desde carpeta local (dtype float32)‚Ä¶")

            with patch(
                "transformers.dynamic_module_utils.get_imports", self._fixed_get_imports
            ):
                # Processor (tokenizer + image processor)
                self.processor = AutoProcessor.from_pretrained(
                    self.model_id, trust_remote_code=True
                )

                # Modelo ‚Äì¬†TODO: usa siempre float32 para coherencia con las entradas
                self.model = (
                    AutoModelForCausalLM.from_pretrained(
                        self.model_id,
                        trust_remote_code=True,
                        use_safetensors=True,
                        torch_dtype=torch.float32,  #  <-- forzamos fp32
                        device_map="auto",
                    )
                    .to(self.device)
                    .eval()
                )

                torch.cuda.empty_cache()

                if callback:
                    callback("‚úÖ Modelo cargado correctamente")
                    if self.device == "cuda":
                        callback(
                            f"üíæ Uso de VRAM: {torch.cuda.memory_allocated() / 1024 ** 3:.1f}¬†GB"
                        )
            return True

        except Exception as exc:
            if callback:
                callback(f"‚ùå Error al cargar modelo: {exc}")
            return False

    # ------------------------------------------------------------------
    #  Liberar recursos
    # ------------------------------------------------------------------
    def descargar_modelo(self):
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None

        if self.device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()

    # ------------------------------------------------------------------
    #  Info de memoria
    # ------------------------------------------------------------------
    def obtener_uso_memoria(self):
        if self.device == "cuda":
            alloc = torch.cuda.memory_allocated() / 1024 ** 3
            reserv = torch.cuda.memory_reserved() / 1024 ** 3
            return f"GPU: {alloc:.1f}¬†GB usado / {reserv:.1f}¬†GB reservado"
        return "Modo¬†CPU"
